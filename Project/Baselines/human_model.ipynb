{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - 1 : Human Assessment\n",
    "\n",
    "The dataset had class imbalance where less than 5% of messages are annotated as lies. The first baseline is reproducing the results and is inspired by the methodology in the ACL 2020 Diplomacy paper. Here, we have established a human baseline by comparing the sender’s intended truthfulness labels with the receiver’s perceived truthfulness labels. The results obtained from our implementation replicate the results from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_class:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None\n",
    "        self.aggregated_messages = None\n",
    "\n",
    "    def load_data(self):\n",
    "        with jsonlines.open(self.file_path, 'r') as reader:\n",
    "            self.data = list(reader)\n",
    "        return self.data\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_bool(label):\n",
    "        convert = lambda x: x if isinstance(x, bool) else True if x.lower() == 'true' else False\n",
    "        return convert(label)\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_label(label):\n",
    "        return label in [True, False, 'true', 'false']\n",
    "\n",
    "    def process_single_dialog(self, dialog):\n",
    "        messages = dialog.get('messages', [])\n",
    "        senders = dialog.get('sender_labels', [])\n",
    "        receivers = dialog.get('receiver_labels', [])\n",
    "        return [{'message': msg, 'sender_annotation': senders[i], 'receiver_annotation': receivers[i]}\n",
    "            for i, msg in enumerate(messages)]\n",
    "\n",
    "    def _aggregate_dialogs(self):\n",
    "        if self.data is None: self.load_data()\n",
    "        return [msg for dialog in self.data for msg in self.process_single_dialog(dialog)]\n",
    "    \n",
    "    def aggregate_data(self):\n",
    "        self.aggregated_messages = self._aggregate_dialogs()\n",
    "        return self.aggregated_messages\n",
    "\n",
    "    def filter_valid_messages(self):\n",
    "        if self.aggregated_messages is None:\n",
    "            self.aggregate_data()\n",
    "        valid_msgs = [{**msg,'sender_annotation': self.convert_to_bool(msg['sender_annotation']),'receiver_annotation': self.convert_to_bool(msg['receiver_annotation'])}\n",
    "            for msg in self.aggregated_messages\n",
    "            if Preprocessing_class.is_valid_label(msg['receiver_annotation'])\n",
    "        ]\n",
    "        return valid_msgs\n",
    "\n",
    "    def get_labels(self):\n",
    "        valid_msgs = self.filter_valid_messages()\n",
    "        sender_labels = [0 if msg['sender_annotation'] else 1 for msg in valid_msgs]\n",
    "        receiver_labels = [0 if msg['receiver_annotation'] else 1 for msg in valid_msgs]\n",
    "        return sender_labels, receiver_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_model_class:\n",
    "    def __init__(self, sender_labels, receiver_labels):\n",
    "        self.sender_labels = sender_labels\n",
    "        self.receiver_labels = receiver_labels\n",
    "\n",
    "    def compute_metric(self, metric_func, **kwargs):\n",
    "        return metric_func(self.sender_labels, self.receiver_labels, **kwargs)\n",
    "\n",
    "    macro_f1 = lambda self: self.compute_metric(f1_score, average='macro', zero_division=0)\n",
    "    lie_f1   = lambda self: self.compute_metric(f1_score, pos_label=1, average='binary', zero_division=0)\n",
    "    accuracy = lambda self: self.compute_metric(accuracy_score)\n",
    "    \n",
    "    def compute_metrics(self):\n",
    "        metrics = {\"Macro F1 Score\": self.macro_f1(),\"Lie F1 Score\": self.lie_f1(), \"Accuracy\": self.accuracy()}\n",
    "        return metrics\n",
    "    \n",
    "    def label_distribution(self):\n",
    "        sender_dist = {label: self.sender_labels.count(label) for label in set(self.sender_labels)}\n",
    "        receiver_dist = {label: self.receiver_labels.count(label) for label in set(self.receiver_labels)}\n",
    "        return sender_dist, receiver_dist\n",
    "\n",
    "    def print_metrics(self):\n",
    "        metrics = self.compute_metrics()\n",
    "        formatted_metrics = {name: round(value, 3) for name, value in metrics.items()}\n",
    "        print(\"Human Model Results : Baseline 1:\")\n",
    "        for name, value in formatted_metrics.items():\n",
    "            print(f\"  {name:<15}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Model Results : Baseline 1:\n",
      "  Macro F1 Score : 0.581\n",
      "  Lie F1 Score   : 0.226\n",
      "  Accuracy       : 0.884\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/varun/Desktop/College/sem6/NLP/Group Project/Data/test.jsonl'\n",
    "\n",
    "dataset_preprocessing = Preprocessing_class(file_path)\n",
    "dataset_preprocessing.load_data()\n",
    "dataset_preprocessing.aggregate_data()\n",
    "sender_labels, receiver_labels = dataset_preprocessing.get_labels()  \n",
    "\n",
    "baseline_1 = Human_model_class(sender_labels, receiver_labels)\n",
    "baseline_1.print_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
